{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all imports related to data engineering\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training json file, then close the file\n",
    "file = open(\"train.json\")\n",
    "raw_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_number_encoding = {\n",
    "    'B-NAME_STUDENT': 0, \n",
    "    'I-NAME_STUDENT': 1, \n",
    "    'B-URL_PERSONAL': 2, \n",
    "    'B-EMAIL': 3, \n",
    "    'B-ID_NUM': 4, \n",
    "    'I-URL_PERSONAL': 5, \n",
    "    'B-USERNAME': 6, \n",
    "    'I-PHONE_NUM': 7,  \n",
    "    'B-STREET_ADDRESS': 8, \n",
    "    'I-STREET_ADDRESS': 9,  \n",
    "    'B-PHONE_NUM': 10, \n",
    "    'I-ID_NUM': 11,\n",
    "    'O': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got the txt file from here: https://gist.github.com/deekayen/4148741\n",
    "\n",
    "# Common words List to hold common words\n",
    "with open(\"most-common-words.txt\", \"r\") as common_words_file:\n",
    "    common_tokens = [word[:-1] for word in list(common_words_file)]\n",
    "    common_tokens.append(\"\\n\\n\")\n",
    "    common_tokens.append(\"\\n\")\n",
    "    common_tokens.append(\" \")\n",
    "\n",
    "# Add punctuation to the list of commonalities\n",
    "for char in list(string.punctuation):\n",
    "    common_tokens.append(char)\n",
    "\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get rid of the 1000 most common used words\n",
    "def common_word_drop(token_list, whitespace_list, label_list, rows):\n",
    "    for word in common_tokens:\n",
    "        if word in token_list:\n",
    "            indices = [i for i, x in enumerate(token_list) if x.lower() == word]\n",
    "            token_list = [token_list[i] for i in range(len(token_list)) if i not in indices]\n",
    "            whitespace_list = [whitespace_list[i] for i in range(len(whitespace_list)) if i not in indices]\n",
    "            label_list = [label_list[i] for i in range(len(label_list)) if i not in indices]\n",
    "            rows = [rows[i] for i in range(len(rows)) if i not in indices]\n",
    "\n",
    "    return token_list, whitespace_list, label_list, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(full_tokens):\n",
    "    rows = []\n",
    "    row_num = 1\n",
    "    for token in full_tokens:\n",
    "        rows.append(row_num)\n",
    "        if token == \"\\n\\n\" or token == \"\\n\":\n",
    "            row_num = row_num + 1\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pii_data_exists(labels):\n",
    "    # Loop over and see if a PII data is found, if it is, return True, else False.\n",
    "    for label in labels:\n",
    "        if label != 'O':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_label(labels):\n",
    "\n",
    "    # Initialize the indexer and for all labels, find the indexes that have PII data\n",
    "    label_indexes = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] != 'O':\n",
    "            label_indexes.append(i)\n",
    "    \n",
    "    # If there is no PII data, if only one PII data, else multiple data\n",
    "    if len(label_indexes) == 0:\n",
    "        return [-1 for label in labels]\n",
    "    elif len(label_indexes) == 1:\n",
    "        label_range = [-1 for label in labels]\n",
    "        label_range[label_indexes[0]] = 0\n",
    "        return label_range\n",
    "    else:\n",
    "        label_range = [-1 for label in labels]\n",
    "\n",
    "        # Get distance for first PII\n",
    "        first_index = label_indexes[0]\n",
    "        first_pii_distance = label_indexes[1] - label_indexes[0]\n",
    "        label_range[first_index] = first_pii_distance\n",
    "\n",
    "        # Get distance for last PII\n",
    "        last_index = label_indexes[-1]\n",
    "        last_pii_distance = label_indexes[-1] - label_indexes[-2]\n",
    "        label_range[last_index] = last_pii_distance\n",
    "        \n",
    "        # Loop over the second to second last PII and get the distances.\n",
    "        for idx in range(1, len(label_indexes)-1):\n",
    "\n",
    "            # For the middle PII data points. Set the previous and next PII\n",
    "            current_pii = label_indexes[idx]\n",
    "            previous_pii = label_indexes[idx-1]\n",
    "            next_pii = label_indexes[idx+1]\n",
    "\n",
    "            # distances\n",
    "            prev_dist = current_pii - previous_pii\n",
    "            next_dist = next_pii - current_pii\n",
    "\n",
    "            # Append the shortest distance to the current pii data\n",
    "            label_range[current_pii] = min(prev_dist, next_dist)\n",
    "        \n",
    "        # Return the label range.\n",
    "        return label_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_data_for_model(data):\n",
    "\n",
    "    # Get the first value from the data\n",
    "    first_doc = data[0]\n",
    "\n",
    "    # Get rid of common words\n",
    "    all_rows = get_rows(first_doc['tokens'])\n",
    "    tokens, white_spaces, labels, rows = common_word_drop(first_doc['tokens'], first_doc['trailing_whitespace'], first_doc['labels'], all_rows)\n",
    "    closest_labels = get_closest_label(labels)\n",
    "\n",
    "    # Create the initial dataframe from the above data\n",
    "    first_doc_data = {\n",
    "        \"tokens\": tokens,\n",
    "        \"trailing_whitespaces\": white_spaces,\n",
    "        \"capitalized first char\": [True if label[0].isupper() else False for label in tokens],\n",
    "        \"token length\": [len(token) for token in tokens],\n",
    "        \"is_numeric\": [True if token.isnumeric() else False for token in tokens],\n",
    "        \"PII label\": [pii_number_encoding[label] for label in labels],\n",
    "        \"Row\": rows,\n",
    "        \"Closest PII data\": closest_labels\n",
    "    }\n",
    "    raw_df = pd.DataFrame(first_doc_data)\n",
    "\n",
    "    # Loop till the end of the data\n",
    "    for document in data[1: len(data) - 1]:\n",
    "\n",
    "        # Check to see if there exists PII data\n",
    "        if not pii_data_exists(document['labels']):\n",
    "            continue\n",
    "            \n",
    "        # Get rid of common words\n",
    "        all_rows = get_rows(document['tokens'])\n",
    "        tokens, white_spaces, labels, rows = common_word_drop(document['tokens'], document['trailing_whitespace'], document['labels'], all_rows)\n",
    "        closest_labels = get_closest_label(labels)\n",
    "\n",
    "        # Collect the data in the same way\n",
    "        doc_data = {\n",
    "            \"tokens\": tokens,\n",
    "            \"trailing_whitespaces\": white_spaces,\n",
    "            \"capitalized first char\": [True if label[0].isupper() else False for label in tokens],\n",
    "            \"token length\": [len(token) for token in tokens],\n",
    "            \"is_numeric\": [True if token.isnumeric() else False for token in tokens],\n",
    "            \"PII label\": [pii_number_encoding[label] for label in labels],\n",
    "            \"Row\": rows,\n",
    "            \"Closest PII data\": closest_labels\n",
    "        }\n",
    "        df = pd.DataFrame(doc_data)\n",
    "\n",
    "        # Concatenate all the data into one single dataframe\n",
    "        raw_df = pd.concat([raw_df, df], ignore_index=True, sort=False)\n",
    "\n",
    "    # Return the concatenated dataframe\n",
    "    return raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data and get rid of some unneeded number values\n",
    "# data = engineer_data_for_model(raw_data[0:6000])\n",
    "data = engineer_data_for_model(raw_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashing_vectorizer = HashingVectorizer(n_features=10, norm=None, alternate_sign=False)\n",
    "\n",
    "encoded_tokens = hashing_vectorizer.transform(data[\"tokens\"]).toarray()\n",
    "\n",
    "encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "hashed_values = [hashlib.sha256(row.tobytes()).hexdigest() for row in encoded_tokens]\n",
    "\n",
    "hashed_integers = [int(hash_val, 16) for hash_val in hashed_values]\n",
    "\n",
    "data[\"hashed_tokens\"] = hashed_integers\n",
    "data.astype({'hashed_tokens': 'int64'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PII_unique = data[\"PII label\"].unique()\n",
    "PII_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test/Train Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "y_train = train_data[\"PII label\"].to_numpy()\n",
    "X_train = train_data.drop(columns=[\"tokens\", \"PII label\"])\n",
    "print(f\"Length train_x = {len(X_train)} \\n Length y_train = {len(y_train)}\")\n",
    "\n",
    "\n",
    "# Test data\n",
    "y_test = test_data[\"PII label\"].to_numpy()\n",
    "X_test = test_data.drop(columns=[\"tokens\", \"PII label\"])\n",
    "print(f\"Length test_x = {len(X_test)} \\n Length test_y = {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create and train the XGBoost Classifier\n",
    "clf = XGBClassifier(n_estimators=100, random_state=42, enable_categorical=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print('Training accuracy:', clf.score(X_train, y_train))\n",
    "print('Test accuracy:', clf.score(X_test, y_test))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
    "print('Cross-Validation Scores:', cv_scores)\n",
    "print('Mean KFold Cross-Validation Accuracy:', cv_scores.mean())\n",
    "\n",
    "# Classification Report\n",
    "y_predictions = clf.predict(X_test)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_predictions)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
