{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gathering Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all imports related to data engineering\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training json file, then close the file\n",
    "file = open(\"raw_data/train.json\")\n",
    "raw_data = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_number_encoding = {\n",
    "    'B-EMAIL': 0, \n",
    "    'B-ID_NUM': 1, \n",
    "    'B-NAME_STUDENT': 2, \n",
    "    'B-PHONE_NUM': 3, \n",
    "    'B-STREET_ADDRESS': 4, \n",
    "    'B-URL_PERSONAL': 5, \n",
    "    'B-USERNAME': 6, \n",
    "    'I-NAME_STUDENT': 7,  \n",
    "    'I-PHONE_NUM': 8, \n",
    "    'I-STREET_ADDRESS': 9,  \n",
    "    'I-URL_PERSONAL': 10, \n",
    "    'O': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', 'you', 'that', 'he', 'was', 'for', 'on', 'are', 'with', 'as', 'I', 'his', 'they', 'be', 'at', 'one', 'have', 'this', 'from', 'or', 'had', 'by', 'not', 'word', 'but', 'what', 'some', 'we', 'can', 'out', 'other', 'were', 'all', 'there', 'when', 'up', 'use', 'your', 'how', 'said', 'an', 'each', 'she', 'which', 'do', 'their', 'time', 'if', 'will', 'way', 'about', 'many', 'then', 'them', 'write', 'would', 'like', 'so', 'these', 'her', 'long', 'make', 'thing', 'see', 'him', 'two', 'has', 'look', 'more', 'day', 'could', 'go', 'come', 'did', 'number', 'sound', 'no', 'most', 'people', 'my', 'over', 'know', 'water', 'than', 'call', 'first', 'who', 'may', 'down', 'side', 'been', 'now', 'find', 'any', 'new', 'work', 'part', 'take', 'get', 'place', 'made', 'live', 'where', 'after', 'back', 'little', 'only', 'round', 'man', 'year', 'came', 'show', 'every', 'good', 'me', 'give', 'our', 'under', 'name', 'very', 'through', 'just', 'form', 'sentence', 'great', 'think', 'say', 'help', 'low', 'line', 'differ', 'turn', 'cause', 'much', 'mean', 'before', 'move', 'right', 'boy', 'old', 'too', 'same', 'tell', 'does', 'set', 'three', 'want', 'air', 'well', 'also', 'play', 'small', 'end', 'put', 'home', 'read', 'hand', 'port', 'large', 'spell', 'add', 'even', 'land', 'here', 'must', 'big', 'high', 'such', 'follow', 'act', 'why', 'ask', 'men', 'change', 'went', 'light', 'kind', 'off', 'need', 'house', 'picture', 'try', 'us', 'again', 'animal', 'point', 'mother', 'world', 'near', 'build', 'self', 'earth', 'father', 'head', 'stand', 'own', 'page', 'should', 'country', 'found', 'answer', 'school', 'grow', 'study', 'still', 'learn', 'plant', 'cover', 'food', 'sun', 'four', 'between', 'state', 'keep', 'eye', 'never', 'last', 'let', 'thought', 'city', 'tree', 'cross', 'farm', 'hard', 'start', 'might', 'story', 'saw', 'far', 'sea', 'draw', 'left', 'late', 'run', \"don't\", 'while', 'press', 'close', 'night', 'real', 'life', 'few', 'north', 'open', 'seem', 'together', 'next', 'white', 'children', 'begin', 'got', 'walk', 'example', 'ease', 'paper', 'group', 'always', 'music', 'those', 'both', 'mark', 'often', 'letter', 'until', 'mile', 'river', 'car', 'feet', 'care', 'second', 'book', 'carry', 'took', 'science', 'eat', 'room', 'friend', 'began', 'idea', 'fish', 'mountain', 'stop', 'once', 'base', 'hear', 'horse', 'cut', 'sure', 'watch', 'color', 'face', 'wood', 'main', 'enough', 'plain', 'girl', 'usual', 'young', 'ready', 'above', 'ever', 'red', 'list', 'though', 'feel', 'talk', 'bird', 'soon', 'body', 'dog', 'family', 'direct', 'pose', 'leave', 'song', 'measure', 'door', 'product', 'black', 'short', 'numeral', 'class', 'wind', 'question', 'happen', 'complete', 'ship', 'area', 'half', 'rock', 'order', 'fire', 'south', 'problem', 'piece', 'told', 'knew', 'pass', 'since', 'top', 'whole', 'king', 'space', 'heard', 'best', 'hour', 'better', 'true', 'during', 'hundred', 'five', 'remember', 'step', 'early', 'hold', 'west', 'ground', 'interest', 'reach', 'fast', 'verb', 'sing', 'listen', 'six', 'table', 'travel', 'less', 'morning', 'ten', 'simple', 'several', 'vowel', 'toward', 'war', 'lay', 'against', 'pattern', 'slow', 'center', 'love', 'person', 'money', 'serve', 'appear', 'road', 'map', 'rain', 'rule', 'govern', 'pull', 'cold', 'notice', 'voice', 'unit', 'power', 'town', 'fine', 'certain', 'fly', 'fall', 'lead', 'cry', 'dark', 'machine', 'note', 'wait', 'plan', 'figure', 'star', 'box', 'noun', 'field', 'rest', 'correct', 'able', 'pound', 'done', 'beauty', 'drive', 'stood', 'contain', 'front', 'teach', 'week', 'final', 'gave', 'green', 'oh', 'quick', 'develop', 'ocean', 'warm', 'free', 'minute', 'strong', 'special', 'mind', 'behind', 'clear', 'tail', 'produce', 'fact', 'street', 'inch', 'multiply', 'nothing', 'course', 'stay', 'wheel', 'full', 'force', 'blue', 'object', 'decide', 'surface', 'deep', 'moon', 'island', 'foot', 'system', 'busy', 'test', 'record', 'boat', 'common', 'gold', 'possible', 'plane', 'stead', 'dry', 'wonder', 'laugh', 'thousand', 'ago', 'ran', 'check', 'game', 'shape', 'equate', 'hot', 'miss', 'brought', 'heat', 'snow', 'tire', 'bring', 'yes', 'distant', 'fill', 'east', 'paint', 'language', 'among', 'grand', 'ball', 'yet', 'wave', 'drop', 'heart', 'am', 'present', 'heavy', 'dance', 'engine', 'position', 'arm', 'wide', 'sail', 'material', 'size', 'vary', 'settle', 'speak', 'weight', 'general', 'ice', 'matter', 'circle', 'pair', 'include', 'divide', 'syllable', 'felt', 'perhaps', 'pick', 'sudden', 'count', 'square', 'reason', 'length', 'represent', 'art', 'subject', 'region', 'energy', 'hunt', 'probable', 'bed', 'brother', 'egg', 'ride', 'cell', 'believe', 'fraction', 'forest', 'sit', 'race', 'window', 'store', 'summer', 'train', 'sleep', 'prove', 'lone', 'leg', 'exercise', 'wall', 'catch', 'mount', 'wish', 'sky', 'board', 'joy', 'winter', 'sat', 'written', 'wild', 'instrument', 'kept', 'glass', 'grass', 'cow', 'job', 'edge', 'sign', 'visit', 'past', 'soft', 'fun', 'bright', 'gas', 'weather', 'month', 'million', 'bear', 'finish', 'happy', 'hope', 'flower', 'clothe', 'strange', 'gone', 'jump', 'baby', 'eight', 'village', 'meet', 'root', 'buy', 'raise', 'solve', 'metal', 'whether', 'push', 'seven', 'paragraph', 'third', 'shall', 'held', 'hair', 'describe', 'cook', 'floor', 'either', 'result', 'burn', 'hill', 'safe', 'cat', 'century', 'consider', 'type', 'law', 'bit', 'coast', 'copy', 'phrase', 'silent', 'tall', 'sand', 'soil', 'roll', 'temperature', 'finger', 'industry', 'value', 'fight', 'lie', 'beat', 'excite', 'natural', 'view', 'sense', 'ear', 'else', 'quite', 'broke', 'case', 'middle', 'kill', 'son', 'lake', 'moment', 'scale', 'loud', 'spring', 'observe', 'child', 'straight', 'consonant', 'nation', 'dictionary', 'milk', 'speed', 'method', 'organ', 'pay', 'age', 'section', 'dress', 'cloud', 'surprise', 'quiet', 'stone', 'tiny', 'climb', 'cool', 'design', 'poor', 'lot', 'experiment', 'bottom', 'key', 'iron', 'single', 'stick', 'flat', 'twenty', 'skin', 'smile', 'crease', 'hole', 'trade', 'melody', 'trip', 'office', 'receive', 'row', 'mouth', 'exact', 'symbol', 'die', 'least', 'trouble', 'shout', 'except', 'wrote', 'seed', 'tone', 'join', 'suggest', 'clean', 'break', 'lady', 'yard', 'rise', 'bad', 'blow', 'oil', 'blood', 'touch', 'grew', 'cent', 'mix', 'team', 'wire', 'cost', 'lost', 'brown', 'wear', 'garden', 'equal', 'sent', 'choose', 'fell', 'fit', 'flow', 'fair', 'bank', 'collect', 'save', 'control', 'decimal', 'gentle', 'woman', 'captain', 'practice', 'separate', 'difficult', 'doctor', 'please', 'protect', 'noon', 'whose', 'locate', 'ring', 'character', 'insect', 'caught', 'period', 'indicate', 'radio', 'spoke', 'atom', 'human', 'history', 'effect', 'electric', 'expect', 'crop', 'modern', 'element', 'hit', 'student', 'corner', 'party', 'supply', 'bone', 'rail', 'imagine', 'provide', 'agree', 'thus', 'capital', \"won't\", 'chair', 'danger', 'fruit', 'rich', 'thick', 'soldier', 'process', 'operate', 'guess', 'necessary', 'sharp', 'wing', 'create', 'neighbor', 'wash', 'bat', 'rather', 'crowd', 'corn', 'compare', 'poem', 'string', 'bell', 'depend', 'meat', 'rub', 'tube', 'famous', 'dollar', 'stream', 'fear', 'sight', 'thin', 'triangle', 'planet', 'hurry', 'chief', 'colony', 'clock', 'mine', 'tie', 'enter', 'major', 'fresh', 'search', 'send', 'yellow', 'gun', 'allow', 'print', 'dead', 'spot', 'desert', 'suit', 'current', 'lift', 'rose', 'continue', 'block', 'chart', 'hat', 'sell', 'success', 'company', 'subtract', 'event', 'particular', 'deal', 'swim', 'term', 'opposite', 'wife', 'shoe', 'shoulder', 'spread', 'arrange', 'camp', 'invent', 'cotton', 'born', 'determine', 'quart', 'nine', 'truck', 'noise', 'level', 'chance', 'gather', 'shop', 'stretch', 'throw', 'shine', 'property', 'column', 'molecule', 'select', 'wrong', 'gray', 'repeat', 'require', 'broad', 'prepare', 'salt', 'nose', 'plural', 'anger', 'claim', 'continent', 'oxygen', 'sugar', 'death', 'pretty', 'skill', 'women', 'season', 'solution', 'magnet', 'silver', 'thank', 'branch', 'match', 'suffix', 'especially', 'fig', 'afraid', 'huge', 'sister', 'steel', 'discuss', 'forward', 'similar', 'guide', 'experience', 'score', 'apple', 'bought', 'led', 'pitch', 'coat', 'mass', 'card', 'band', 'rope', 'slip', 'win', 'dream', 'evening', 'condition', 'feed', 'tool', 'total', 'basic', 'smell', 'valley', 'nor', 'double', 'seat', 'arrive', 'master', 'track', 'parent', 'shore', 'division', 'sheet', 'substance', 'favor', 'connect', 'post', 'spend', 'chord', 'fat', 'glad', 'original', 'share', 'station', 'dad', 'bread', 'charge', 'proper', 'bar', 'offer', 'segment', 'slave', 'duck', 'instant', 'market', 'degree', 'populate', 'chick', 'dear', 'enemy', 'reply', 'drink', 'occur', 'support', 'speech', 'nature', 'range', 'steam', 'motion', 'path', 'liquid', 'log', 'meant', 'quotient', 'teeth', 'shell', 'nec', '\\n\\n', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Got the txt file from here: https://gist.github.com/deekayen/4148741\n",
    "\n",
    "# Common words List to hold common words\n",
    "with open(\"ml-data-input/most-common-words.txt\", \"r\") as common_words_file:\n",
    "    common_tokens = [word[:-1] for word in list(common_words_file)]\n",
    "    common_tokens.append(\"\\n\\n\")\n",
    "    common_tokens.append(\"\\n\")\n",
    "    common_tokens.append(\" \")\n",
    "\n",
    "# Add punctuation to the list of commonalities\n",
    "for char in list(string.punctuation):\n",
    "    common_tokens.append(char)\n",
    "\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get rid of the 1000 most common used words\n",
    "def common_word_drop(token_list, whitespace_list, label_list):\n",
    "    for word in common_tokens:\n",
    "        if word in token_list:\n",
    "            indices = [i for i, x in enumerate(token_list) if x.lower() == word]\n",
    "            token_list = [token_list[i] for i in range(len(token_list)) if i not in indices]\n",
    "            whitespace_list = [whitespace_list[i] for i in range(len(whitespace_list)) if i not in indices]\n",
    "            label_list = [label_list[i] for i in range(len(label_list)) if i not in indices]\n",
    "\n",
    "    return token_list, whitespace_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_data_for_model(data):\n",
    "\n",
    "    # Get the first value from the data\n",
    "    first_doc = data[0]\n",
    "\n",
    "    # Get rid of common words\n",
    "    # tokens, white_spaces, labels = common_word_drop(first_doc['tokens'], first_doc['trailing_whitespace'], first_doc['labels'])\n",
    "    tokens, white_spaces, labels = common_word_drop(first_doc['tokens'], first_doc['trailing_whitespace'], first_doc['labels'])\n",
    "\n",
    "    # Create the initial dataframe from the above data\n",
    "    first_doc_data = {\n",
    "        \"tokens\": tokens,\n",
    "        \"trailing_whitespaces\": white_spaces,\n",
    "        \"capitalized first char\": [True if label[0].isupper() else False for label in tokens],\n",
    "        \"token length\": [len(token) for token in tokens],\n",
    "        \"is_numeric\": [True if token.isnumeric() else False for token in tokens],\n",
    "        \"PII label\": [pii_number_encoding[label] for label in labels]\n",
    "    }\n",
    "    raw_df = pd.DataFrame(first_doc_data)\n",
    "\n",
    "    # Loop till the end of the data\n",
    "    for document in data[1: len(data) - 1]:\n",
    "\n",
    "        # Get rid of common words\n",
    "        # tokens, white_spaces, labels = common_word_drop(document['tokens'], document['trailing_whitespace'], document['labels'])\n",
    "        tokens, white_spaces, labels = common_word_drop(document['tokens'], document['trailing_whitespace'], document['labels'])\n",
    "\n",
    "        # Collect the data in the same way\n",
    "        doc_data = {\n",
    "            \"tokens\": tokens,\n",
    "            \"trailing_whitespaces\": white_spaces,\n",
    "            \"capitalized first char\": [True if label[0].isupper() else False for label in tokens],\n",
    "            \"token length\": [len(token) for token in tokens],\n",
    "            \"is_numeric\": [True if token.isnumeric() else False for token in tokens],\n",
    "            \"PII label\": [pii_number_encoding[label] for label in labels]\n",
    "        }\n",
    "        df = pd.DataFrame(doc_data)\n",
    "\n",
    "        # Concatenate all the data into one single dataframe\n",
    "        raw_df = pd.concat([raw_df, df], ignore_index=True, sort=False)\n",
    "\n",
    "    # Return the concatenated dataframe\n",
    "    return raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokens</th>\n",
       "      <th>trailing_whitespaces</th>\n",
       "      <th>capitalized first char</th>\n",
       "      <th>token length</th>\n",
       "      <th>is_numeric</th>\n",
       "      <th>PII label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thinking</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>innovation</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Avril</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28295</th>\n",
       "      <td>28295</td>\n",
       "      <td>reality</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28296</th>\n",
       "      <td>28296</td>\n",
       "      <td>because</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>28297</td>\n",
       "      <td>already</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28298</th>\n",
       "      <td>28298</td>\n",
       "      <td>creating</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28299</th>\n",
       "      <td>28299</td>\n",
       "      <td>innovation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28300 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      tokens  trailing_whitespaces  capitalized first char  \\\n",
       "0          0    Thinking                  True                    True   \n",
       "1          1  innovation                  True                   False   \n",
       "2          2   reflexion                 False                   False   \n",
       "3          3       Avril                  True                    True   \n",
       "4          4        2021                 False                   False   \n",
       "...      ...         ...                   ...                     ...   \n",
       "28295  28295     reality                 False                   False   \n",
       "28296  28296     because                  True                   False   \n",
       "28297  28297     already                  True                   False   \n",
       "28298  28298    creating                  True                   False   \n",
       "28299  28299  innovation                 False                   False   \n",
       "\n",
       "       token length  is_numeric  PII label  \n",
       "0                 8       False         11  \n",
       "1                10       False         11  \n",
       "2                 9       False         11  \n",
       "3                 5       False         11  \n",
       "4                 4        True         11  \n",
       "...             ...         ...        ...  \n",
       "28295             7       False         11  \n",
       "28296             7       False         11  \n",
       "28297             7       False         11  \n",
       "28298             8       False         11  \n",
       "28299            10       False         11  \n",
       "\n",
       "[28300 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training data and get rid of some unneeded number values\n",
    "data = engineer_data_for_model(raw_data[0:20]).reset_index()\n",
    "\n",
    "# Show the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors='ignore')\n",
    "\n",
    "def get_token_vector(token):\n",
    "  try:\n",
    "    return word2vec_model[token]\n",
    "  except KeyError:\n",
    "    return np.zeros(word2vec_model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.2421875, 0.14550781, 0.026855469, 0.00759...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.22558594, -0.01953125, 0.09082031, 0.2373...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.30078125, 0.18945312, -0.03491211, 0.125,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.10595703, 0.21386719, 0.118652344, -0.031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.106933594, -0.10546875, 0.053222656, 0.069...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28295</th>\n",
       "      <td>[[-0.30078125, 0.18945312, -0.03491211, 0.125,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28296</th>\n",
       "      <td>[[-0.203125, 0.053222656, 0.109375, 0.21777344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28298</th>\n",
       "      <td>[[-0.20800781, 0.034179688, 0.025756836, 0.179...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28299</th>\n",
       "      <td>[[-0.22558594, -0.01953125, 0.09082031, 0.2373...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 vectors\n",
       "0      [[-0.2421875, 0.14550781, 0.026855469, 0.00759...\n",
       "1      [[-0.22558594, -0.01953125, 0.09082031, 0.2373...\n",
       "2      [[-0.30078125, 0.18945312, -0.03491211, 0.125,...\n",
       "3      [[-0.10595703, 0.21386719, 0.118652344, -0.031...\n",
       "4      [[0.106933594, -0.10546875, 0.053222656, 0.069...\n",
       "...                                                  ...\n",
       "28295  [[-0.30078125, 0.18945312, -0.03491211, 0.125,...\n",
       "28296  [[-0.203125, 0.053222656, 0.109375, 0.21777344...\n",
       "28297  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "28298  [[-0.20800781, 0.034179688, 0.025756836, 0.179...\n",
       "28299  [[-0.22558594, -0.01953125, 0.09082031, 0.2373...\n",
       "\n",
       "[28300 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_data = pd.DataFrame({\n",
    "  'vectors': data['tokens'].apply(lambda tokens: [get_token_vector(token) for token in tokens])\n",
    "})\n",
    "\n",
    "vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [-0.2421875, 0.14550781, 0.026855469, 0.007598...\n",
       "1        [-0.2255859375, -0.01953125, 0.0908203125, 0.2...\n",
       "2        [-0.30078125, 0.18945312, -0.03491211, 0.125, ...\n",
       "3        [-0.10595703, 0.21386719, 0.118652344, -0.0314...\n",
       "4        [0.106933594, -0.10546875, 0.053222656, 0.0698...\n",
       "                               ...                        \n",
       "28295    [-0.30078125, 0.189453125, -0.034912109375, 0....\n",
       "28296    [-0.203125, 0.05322265625, 0.109375, 0.2177734...\n",
       "28297    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "28298    [-0.2080078125, 0.0341796875, 0.0257568359375,...\n",
       "28299    [-0.2255859375, -0.01953125, 0.0908203125, 0.2...\n",
       "Name: vectors, Length: 28300, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_matrix(matrix):\n",
    "    return np.ravel(matrix)\n",
    "\n",
    "flattened_data = vectorized_data[\"vectors\"].apply(flatten_matrix)\n",
    "\n",
    "flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "max_len = max(flattened_data.apply(len))\n",
    "\n",
    "padded_sequences = flattened_data.apply(lambda vectors: list(zip_longest(vectors, fillvalue=np.zeros_like(vectors[0]))))\n",
    "\n",
    "vectorized_data_padded = pd.DataFrame(padded_sequences.tolist(), columns=[f'feature_{i}' for i in range(max_len)])\n",
    "\n",
    "print(vectorized_data_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the flattened features with the labels\n",
    "final_data = pd.concat([flattened_data, data['PII label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import hashlib\n",
    "\n",
    "#hashed_values = [hashlib.sha256(row.tobytes()).hexdigest() for row in encoded_tokens]\n",
    "\n",
    "#hashed_integers = [int(hash_val, 16) for hash_val in hashed_values]\n",
    "\n",
    "# data[\"hashed_tokens\"] = hashed_integers\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "y_train = train_data[\"PII label\"].to_numpy()\n",
    "x_train = train_data.drop(columns=[\"tokens\", \"index\", \"PII label\"])\n",
    "\n",
    "\n",
    "# Test data\n",
    "y_test = test_data[\"PII label\"].to_numpy()\n",
    "x_test = test_data.drop(columns=[\"tokens\", \"index\", \"PII label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length train_x = {len(x_train)} \\n Length train_y = {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length test_x = {len(x_test)} \\n Length test_y = {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed classes\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='sag', class_weight='balanced')\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Text(0.5,257.44,'Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'B-EMAIL': 0, \n",
    "    'B-ID_NUM': 1, \n",
    "    'B-NAME_STUDENT': 2, \n",
    "    'B-PHONE_NUM': 3, \n",
    "    'B-STREET_ADDRESS': 4, \n",
    "    'B-URL_PERSONAL': 5, \n",
    "    'B-USERNAME': 6, \n",
    "    'I-NAME_STUDENT': 7,  \n",
    "    'I-PHONE_NUM': 8, \n",
    "    'I-STREET_ADDRESS': 9,  \n",
    "    'I-URL_PERSONAL': 10, \n",
    "    'O': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = list(pii_number_encoding.keys())\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
